{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d26f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46545466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>書名</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>jieba_cut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>詭圖：地圖歷史上最偉大的神話、謊言和謬誤（精裝）</td>\n",
       "      <td>9789869590235</td>\n",
       "      <td>全彩,高度,教授,博物,荒野,副教授,排行榜,未知,困境,揭露,大地,征服,探索,主人,整理...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>瞄過一眼就忘不了的世界史：高中老師╳神級YouTuber 2，000萬次點閱的超人氣課程</td>\n",
       "      <td>9789863844433</td>\n",
       "      <td>世界史,神級,考前,年表,死背,事件,高分,故事,好看,特色,分章,整理,不用,瞄過,狂銷,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>忘了自己是動物的人類：重思生命起源的歷史與身而為人的意義</td>\n",
       "      <td>9789864779376</td>\n",
       "      <td>認為,身為,台灣,起源,思考,星球,人性,狂想,故事,義荷,葡俄西,視為,將人,將身,作為,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>大戰略：耶魯大學長紅20年大師課程，從歷史提煉的領導決策心法</td>\n",
       "      <td>9789570856712</td>\n",
       "      <td>狐狸,甘迺迪,掌握,學程,失準,政策,心法,探看,政治,全局,跨越,同事,思考,方式,原本,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021世界年鑑</td>\n",
       "      <td>9789869278799</td>\n",
       "      <td>疫情,全球,大事,反送,肆虐,入主,肺炎,世界,深入,整理,香港,全面,控制,北京</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             書名           ISBN  \\\n",
       "0                      詭圖：地圖歷史上最偉大的神話、謊言和謬誤（精裝）  9789869590235   \n",
       "1  瞄過一眼就忘不了的世界史：高中老師╳神級YouTuber 2，000萬次點閱的超人氣課程  9789863844433   \n",
       "2                  忘了自己是動物的人類：重思生命起源的歷史與身而為人的意義  9789864779376   \n",
       "3                大戰略：耶魯大學長紅20年大師課程，從歷史提煉的領導決策心法  9789570856712   \n",
       "4                                      2021世界年鑑  9789869278799   \n",
       "\n",
       "                                           jieba_cut  \n",
       "0  全彩,高度,教授,博物,荒野,副教授,排行榜,未知,困境,揭露,大地,征服,探索,主人,整理...  \n",
       "1  世界史,神級,考前,年表,死背,事件,高分,故事,好看,特色,分章,整理,不用,瞄過,狂銷,...  \n",
       "2  認為,身為,台灣,起源,思考,星球,人性,狂想,故事,義荷,葡俄西,視為,將人,將身,作為,...  \n",
       "3  狐狸,甘迺迪,掌握,學程,失準,政策,心法,探看,政治,全局,跨越,同事,思考,方式,原本,...  \n",
       "4          疫情,全球,大事,反送,肆虐,入主,肺炎,世界,深入,整理,香港,全面,控制,北京  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_json('./jieba_top40.json',lines=True)\n",
    "del df['_id']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f36ea33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把str轉成list \n",
    "df['jieba_list']=df.jieba_cut.apply(lambda x:x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c5b27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      書名           ISBN  \\\n",
      "193314  長生外傳：霸道總裁愛上我 （上）  9789865051112   \n",
      "\n",
      "                                               jieba_cut  \\\n",
      "193314  霸道,陌生人,好像,初吻,前求,作為,存封,心扉,溜走,自持,敲打,眼神,直至,不成,男人,建立   \n",
      "\n",
      "                                               jieba_list  \n",
      "193314  [霸道, 陌生人, 好像, 初吻, 前求, 作為, 存封, 心扉, 溜走, 自持, 敲打, ...  \n",
      "             書名           ISBN  \\\n",
      "215304  暖男是霸道總裁  9789866959684   \n",
      "\n",
      "                                                jieba_cut  \\\n",
      "215304  霸道,迷妹,搞不定,軟音,教林,洛遙,妒火,色狼,女人,醋意,家世,上床,供奉,好人,所有人...   \n",
      "\n",
      "                                               jieba_list  \n",
      "215304  [霸道, 迷妹, 搞不定, 軟音, 教林, 洛遙, 妒火, 色狼, 女人, 醋意, 家世, ...  \n"
     ]
    }
   ],
   "source": [
    "# 隨機找兩本書\n",
    "print(df[df.ISBN=='9789865051112'])\n",
    "print(df[df.ISBN=='9789866959684'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10419317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結合全部jieba_list\n",
    "jieba_list=df.jieba_list.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd1ec3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 建立模型\n",
    "# model = Word2Vec(jieba_list, vector_size=100,min_count=1,workers=7, window =3, sg = 1) #size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96053cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存出 存取模型\n",
    "# model = Word2Vec.save(\"word2vec_book_gensim.model\")\n",
    "model = Word2Vec.load(\"word2vec_book_gensim.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c06b3625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['霸道', '迷妹', '搞不定', '軟音', '教林', '洛遙', '妒火', '色狼', '女人', '醋意', '家世', '上床', '供奉', '好人', '所有人', '男人', '直到', '行情', '知道', '公司']\n",
      "2, ['霸道', '陌生人', '好像', '初吻', '前求', '作為', '存封', '心扉', '溜走', '自持', '敲打', '眼神', '直至', '不成', '男人', '建立']\n"
     ]
    }
   ],
   "source": [
    "# 找書本的斷字\n",
    "exp_sent_1 = df.iloc[215304,3]\n",
    "print('1',exp_sent_1)\n",
    "exp_sent_2 = df.iloc[193314,3]\n",
    "print('2,',exp_sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99f660df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.02725030e+00  7.03955087e+00 -1.49198057e+00 -6.72144890e-01\n",
      " -2.66756507e+00 -5.17977493e+00 -3.10132700e+00  6.99134423e+00\n",
      "  2.12182422e+00 -7.68259013e+00 -5.11376140e+00 -8.40786897e+00\n",
      "  1.66318772e+00  4.39769515e+00 -4.32891173e+00 -2.35308059e+00\n",
      "  2.75160858e-01 -9.46365687e+00  7.46471662e+00 -6.30268244e+00\n",
      "  2.31593362e+00  5.79780547e+00  1.47046564e+00 -1.47844299e+00\n",
      "  1.20648601e+00 -9.38862221e-01  3.47412499e+00 -3.99062429e+00\n",
      " -7.25514384e+00  2.54258661e+00  5.31477182e+00  3.50942210e+00\n",
      " -3.06152480e+00 -4.84001214e+00 -6.45091383e-01  8.92441730e+00\n",
      "  1.83267482e+00 -2.61858866e+00 -5.62139381e+00 -1.32110786e+01\n",
      "  6.50513341e+00 -6.96400728e+00 -8.94743726e+00 -2.36209434e+00\n",
      "  8.02508930e+00  2.80890391e+00 -6.11586090e+00 -5.23759061e+00\n",
      " -2.77174864e-01  1.61532532e-01 -9.56118444e-01 -4.48503907e+00\n",
      " -1.09481467e+01 -3.78766339e-02 -5.66770586e+00 -2.53836874e+00\n",
      "  5.07409802e+00 -1.32249742e+00 -9.38024569e+00  6.94497863e+00\n",
      "  8.71100404e+00 -5.12421309e+00  8.94370004e+00 -5.66702170e-01\n",
      " -4.97445697e+00  2.77757940e+00  5.97300384e+00  1.75473935e+00\n",
      "  2.83001998e+00  1.17409201e+01  7.11037558e+00  8.36670585e-03\n",
      "  6.74893186e-01 -3.50602541e+00  2.68807617e+00  3.82898692e+00\n",
      "  1.67864426e+00 -6.63771968e+00 -4.16698144e+00  2.61121944e+00\n",
      " -6.14772063e+00  3.95299496e+00 -1.06360214e+01  9.17946860e+00\n",
      " -2.21759104e+00 -5.89950950e-01  3.51599761e+00  4.10005446e+00\n",
      "  5.63385595e+00 -7.96021320e-01 -5.54074496e-02  4.48944182e+00\n",
      "  2.87981334e+00  1.28214992e+00 -1.22788200e+00  7.41156158e+00\n",
      "  2.55981366e+00 -2.77226304e+00 -1.50909987e+00  1.90308567e-01]\n",
      "[  0.18399928   3.87002286   0.71316318  -3.12634547  -1.18106641\n",
      "  -4.8555868   -2.17210681   6.03814995   2.40893384  -7.40436057\n",
      "  -6.2281923   -7.35968283   0.17979681   1.76102984  -6.41605945\n",
      "  -3.15396437   1.16718444  -9.35037299   5.03864712  -6.67037278\n",
      "   0.50312656   6.28662559   1.0956984   -0.70612025   0.9600294\n",
      "   2.64275743   1.65885007  -3.55432155  -4.36654253   1.16302954\n",
      "   5.29497844   2.61576676  -4.53111725  -3.08178856  -0.92389281\n",
      "   7.74667257   0.78537998  -4.47333789  -6.53976962 -12.3434564\n",
      "   5.27368882  -7.26848438  -9.66380359  -2.05149571   6.75685155\n",
      "   1.61041945  -6.32013601  -3.43292331  -0.8555117   -0.76448148\n",
      "  -1.66115492  -2.92201746  -8.45225422  -0.71609062  -5.7561203\n",
      "  -1.81571622   3.30557513  -2.49969248  -7.54671175   3.40470849\n",
      "   7.03334619  -5.7812342    6.97250639  -1.45864885  -2.38120111\n",
      "   2.69746133   3.65137992   1.1071777    3.31163432   9.48284902\n",
      "   3.87689296   2.0497921    3.79268374  -0.99003697  -0.30686644\n",
      "   2.62950017   1.84342643  -2.77823515  -3.67223149   1.13862452\n",
      "  -6.35135726   3.84863406 -10.83767045   7.46193834  -1.71762107\n",
      "  -0.86765898   1.50039046   4.99219852   5.09323458  -0.25251095\n",
      "   0.85173642   4.01227324   2.97625136   2.6322091   -1.3093023\n",
      "   5.090135     0.42281266  -1.5892772   -1.79308009   0.41187103]\n"
     ]
    }
   ],
   "source": [
    "# 統一維度\n",
    "vec_sent_1_w2v = np.zeros(100)\n",
    "vec_sent_2_w2v = np.zeros(100)\n",
    "for w in exp_sent_1:\n",
    "    vec_sent_1_w2v += model.wv[w]\n",
    "print(vec_sent_1_w2v)\n",
    "for w in exp_sent_2:\n",
    "    vec_sent_2_w2v += model.wv[w]\n",
    "print(vec_sent_2_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5a0d2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity of Word2Vec+:  0.957078872981339\n"
     ]
    }
   ],
   "source": [
    "# 兩本書的相似度\n",
    "w2v_similarity = np.dot(vec_sent_1_w2v, vec_sent_2_w2v) / (np.linalg.norm(vec_sent_1_w2v)*np.linalg.norm(vec_sent_2_w2v))\n",
    "print('similarity of Word2Vec+: ', w2v_similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
